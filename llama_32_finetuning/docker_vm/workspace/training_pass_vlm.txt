Multi-GPU Vision Language Model Finetuning Script Instructions

This script enables distributed training of the vision-language model across all available GPUs using unsloth's built-in multi-GPU support.

Basic Usage:
python multigpu_finetune_vlm.py

Optional Arguments:
--data: Specify a custom dataset (default: unsloth/Radiology_mini)
--instruction: Custom instruction prompt for the model (default: "You are an expert radiographer. Describe accurately what you see in this image.")

Examples:
1. Run with default settings (automatically uses all available GPUs):
python multigpu_finetune_vlm.py

2. Run with custom dataset:
python multigpu_finetune_vlm.py --data "your/dataset/name"

3. Run with custom instruction:
python multigpu_finetune_vlm.py --instruction "Describe what you see in this medical image in detail."

4. Run with both custom dataset and instruction:
python multigpu_finetune_vlm.py --data "your/dataset/name" --instruction "Your custom instruction here"

Notes:
- Automatically detects and uses all available GPUs
- Uses unsloth's device_map="auto" for optimal GPU memory distribution
- Automatically scales batch size and gradient accumulation with number of GPUs
- All runs are logged to the public "hack_oslo" project in Weights & Biases
- Each run is named with the current date/time in Danish format: DD-MM-YYYY_HH-MM-SS
- The model uses 4-bit quantization to reduce memory usage
- Training progress and metrics can be monitored in real-time on W&B dashboard

Requirements:
- torch with CUDA support
- transformers
- unsloth
- wandb
- datasets
- trl
- One or more CUDA-capable GPUs

Memory Requirements:
- Uses 4-bit quantization to minimize memory footprint
- Memory is automatically distributed across available GPUs using unsloth's device mapping 